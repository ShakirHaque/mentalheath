import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, f1_score
from sklearn.pipeline import Pipeline

# Load the training and test datasets
train_df = pd.read_csv('train_data_mental.csv')
test_df = pd.read_csv('test_data_mental.csv')

# Handle missing values
train_df.dropna(subset=['statement', 'status'], inplace=True)
test_df.dropna(subset=['statement', 'status'], inplace=True)

train_df['statement'].fillna('', inplace=True)
test_df['statement'].fillna('', inplace=True)

# Convert labels to numeric values
label_mapping = {label: idx for idx, label in enumerate(train_df['status'].unique())}
train_df['label'] = train_df['status'].map(label_mapping)
test_df['label'] = test_df['status'].map(label_mapping)

# Extract features and labels
X_train = train_df['statement']
y_train = train_df['label']
X_test = test_df['statement']
y_test = test_df['label']

# Create a pipeline
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(max_features=5000)),
    ('model', MultinomialNB())
])

# Train the pipeline
pipeline.fit(X_train, y_train)

# Make predictions
y_pred = pipeline.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f'Overall Accuracy: {accuracy * 100:.2f}%')

print('\nClassification Report:')
print(classification_report(y_test, y_pred, target_names=label_mapping.keys()))

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Plotting Confusion Matrix
plt.figure(figsize=(10, 7))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=label_mapping.keys(), yticklabels=label_mapping.keys())
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.title('Confusion Matrix')
plt.show()

# Calculating Precision, Recall, and F1-Score for Each Class
precision = precision_score(y_test, y_pred, average=None)
recall = recall_score(y_test, y_pred, average=None)
f1 = f1_score(y_test, y_pred, average=None)

# Plotting Precision, Recall, and F1-Score
metrics_df = pd.DataFrame({
    'Class': label_mapping.keys(),
    'Precision': precision,
    'Recall': recall,
    'F1 Score': f1
})

metrics_df.set_index('Class', inplace=True)
metrics_df.plot(kind='bar', figsize=(12, 6))
plt.title('Precision, Recall, and F1 Score per Class')
plt.ylabel('Score')
plt.xticks(rotation=45)
plt.grid(axis='y')
plt.show()
